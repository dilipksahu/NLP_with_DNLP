{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Vocabulary_&_Matching.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uENjwf_oAtNF"
      },
      "source": [
        "# Vocabulary and Matching\n",
        "So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
        "\n",
        "Lets understand about identify and label specific phrases that match patterns in NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwBXtpFPAtNG"
      },
      "source": [
        "## Rule-based Matching\n",
        "spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher.\n",
        "For more: https://spacy.io/usage/linguistic-features#section-rule-based-matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xd_UfhzAtNH"
      },
      "source": [
        "# Perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-7ylMnzAtNP"
      },
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sqBC_cDAtNU"
      },
      "source": [
        "<font color=green>Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mo4HJj1AtNV"
      },
      "source": [
        "### Creating patterns\n",
        "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az1f4M8EAtNY"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
        "\n",
        "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BszDc5GHAtNd"
      },
      "source": [
        "Let's break this down:\n",
        "* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n",
        "* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
        "* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n",
        "\n",
        "<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n",
        "<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yYuncGXAtNe"
      },
      "source": [
        "### Applying the matcher to a Doc object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0jo-uXbAtNf"
      },
      "source": [
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ32cChlAtNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf11531-d0b4-4223-ce7e-fd3b4711b64d"
      },
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdyVZJBXAtNr"
      },
      "source": [
        "`matcher` returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span `doc[start:end]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl_b6NtTAtNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718301fb-8692-4b0d-a9a5-1306430a4323"
      },
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8656102463236116519 SolarPower 1 3 Solar Power\n",
            "8656102463236116519 SolarPower 10 11 solarpower\n",
            "8656102463236116519 SolarPower 13 16 Solar-power\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56-nFNUAtNy"
      },
      "source": [
        "The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7i5-S2CAtNz"
      },
      "source": [
        "### Setting pattern options and quantifiers\n",
        "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adJqxtdaAtN0"
      },
      "source": [
        "# Redefine the patterns:\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuBTZbgFAtN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715813e0-adc3-4f11-c8ee-d3b353048ced"
      },
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxeZlvBGAtN9"
      },
      "source": [
        "This found both two-word patterns, with and without the hyphen!\n",
        "\n",
        "The following quantifiers can be passed to the `'OP'` key:\n",
        "<table><tr><th>OP</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
        "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
        "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
        "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S3Vx72bAtN-"
      },
      "source": [
        "### Be careful with lemmas!\n",
        "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApJufoJmAtN_"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcV2pOUbAtOF"
      },
      "source": [
        "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMvZo6AhAtOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a6f831-2ac0-4cac-9ed7-e5a91804d968"
      },
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IniRRPXDAtOO"
      },
      "source": [
        "<font color=green>The matcher found the first occurrence because the lemmatizer treated 'Solar-powered' as a verb, but not the second as it considered it an adjective.<br>For this case it may be better to set explicit token patterns.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg6huqwwAtOP"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solarpowered'}]\n",
        "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2, pattern3, pattern4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxZVPb4PAtOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a9c3d7-3997-4be7-aff3-1d135d1e1de6"
      },
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCFezAj_AtOa"
      },
      "source": [
        "## Other token attributes\n",
        "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
        "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
        "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
        "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
        "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
        "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
        "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
        "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
        "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
        "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6qWHdtyAtOa"
      },
      "source": [
        "### Token wildcard\n",
        "You can pass an empty dictionary `{}` as a wildcard to represent **any token**. For example, you might want to retrieve hashtags without knowing what might follow the `#` character:\n",
        ">`[{'ORTH': '#'}, {}]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qziLAi0OAtOb"
      },
      "source": [
        "___\n",
        "## PhraseMatcher\n",
        "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm6vPOYbAtOc"
      },
      "source": [
        "# Perform standard imports, reset nlp\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB6DaZVsAtOh"
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nqGIvuNErly"
      },
      "source": [
        "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
        "patterns = [nlp(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", None, *patterns)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VccJ1kqBiTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8119629f-cd0f-4007-e63e-4c5b8708f3ea"
      },
      "source": [
        "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
        "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
        "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
        "               \"Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\") \n",
        "matches = matcher(text_doc)\n",
        "print(matches)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSvSV3JkJAXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fba28a9-ffbb-41d5-cae7-15c759c3ef4e"
      },
      "source": [
        "match_id, start, end = matches[0]\n",
        "print(nlp.vocab.strings[match_id], text_doc[start:end])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TerminologyList iPhone 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN-0cgHjJAms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea68525-c8c8-42dd-f32c-00a8ac28c9c4"
      },
      "source": [
        "for match_id, start, end in matches:\r\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\r\n",
        "    span = text_doc[start:end]                    # get the matched span\r\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3766102292120407359 TerminologyList 17 19 iPhone 11\n",
            "3766102292120407359 TerminologyList 22 24 Galaxy Note\n",
            "3766102292120407359 TerminologyList 30 32 iPhone XS\n",
            "3766102292120407359 TerminologyList 33 35 Google Pixel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGdf77kg8zjP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}